<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chengxu Liu</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Chengxu Liu</name>
              </p>
              <p>I am a 3rd-year Ph.D. student at <a href="http://www.xjtu.edu.cn/">Xi'an Jiaotong University</a> (XJTU), My advisors are <a href="https://scholar.google.com/citations?user=skQCiQQAAAAJ&hl=zh-CN">Xueming Qian</a>. I received the B.S. degree from <a href= "http://dice.xjtu.edu.cn/">School of the Information engineering</a>, XJTU in 2019. 
              </p>
              <p>
                I was fortunate to be involved in internship program at <a href="https://en.megvii.com/megvii_research">Megvii Research</a>, <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia</a> (MSRA).
              </p>
              <p style="text-align:center">
                <a href="mailto:chengxuliu@stu.xjtu.edu.cn">Email</a> &nbsp/&nbsp
                <a href="data/CV.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=6Z7kVgoAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                <a href="https://www.linkedin.com/in/%E6%88%90%E6%97%AD-%E5%88%98-421ba11b8/">LinkedIn</a>&nbsp/&nbsp
                <a href="https://github.com/ChengxuLiu/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/chengxu.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/chengxu.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
            <hr>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
		        <li><b>[2024/01]</b> One paper accepted by <b>AAAI 2024</b>.</li>
                        <li><b>[2023/09]</b> One paper accepted by <b>IEEE TII</b>.</li>
                        <li><b>[2023/08]</b> One paper accepted by <b>IEEE TIP</b>.</li>
                        <li><b>[2023/07]</b> Two papers accepted by <b>ICCV 2023</b>.</li>
                        <li><b>[2023/06]</b> One paper accepted by <b>IEEE TIP</b>.</li>
                        <li><b>[2023/05]</b> One paper accepted by <b>IEEE TCSVT</b>.</li>
                        <li><b>[2023/04]</b> One paper accepted by <b>KBS</b>.</li>
                        <li><b>[2022/06]</b> One paper accepted by <b>IEEE TNNLS</b>.</li>
                        <li><b>[2022/03]</b> One <font color="red"><b>oral</b></font> paper accepted by <b>CVPR 2022</b>.</li>
                        <li><b>[2021/08]</b> One paper accepted by <b>IEEE TCSVT</b>.</li>
            </td>
          </tr>
        </tbody></table>
            <hr>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in image/video restoration, low-quality/degraded detection, and fine-grained recognition. Much of my research is about low-level vision. Representative papers are <span class="highlight">highlighted</span>. (* indicates equal contribution)
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					
          <tr> 
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/Split-Check.png"><img src="images/Split-Check.png" alt="Split-Check" width="256" height="160"></a>
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/10248957">
                <papertitle>Split-Check: Boosting Product Recognition via Instance-Level Retrieval</papertitle>
              </a>
              <br>
              <strong>Chengxu Liu</strong>, 
              <a href="https://www.aminer.org/profile/zongyang-da/6447f4b9e3c28ee84c23b312">Zongyang Da</a>,
              <a href="https://www.semanticscholar.org/author/Yuanzhi-Liang/72322311">Yuanzhi Liang</a>,
              <a href="https://gr.xjtu.edu.cn/en/web/xueyao">Yao Xue</a>,
              <a href="http://guoshuaizhao.com/">Guoshuai Zhao</a>,
              <a href="https://scholar.google.com/citations?user=skQCiQQAAAAJ&hl=zh-CN">Xueming Qian</a>
              <br>
              <em>IEEE TII</em>, 2023
              <br>
              <!-- <a href="https://ieeexplore.ieee.org/abstract/document/10248957">[PDF]</a> -->
              <!-- <a href="https://arxiv.org/abs/2211.08887">[arXiv]</a> -->
	            <!-- <a href="https://github.com/OpenPerceptionX/maskalign">[Code]</a> -->
              <p>We propose a product recognition approach based on intelligent UVMs, called Split-Check, which first splits the region of interest of products by detection and then check product by instance-level retrieval.</p>
            </td>
          </tr>

          <tr bgcolor="#ffffd0"> 
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/FSI.png"><img src="images/FSI.png" alt="FSI" width="256" height="160"></a>
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>FSI: Frequency and Spatial Interactive Learning for Image Restoration in Under-Display Cameras</papertitle>
              </a>
              <br>
              <strong>Chengxu Liu</strong>, 
              <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=pLHwzNwAAAAJ">Xuan Wang</a>, 
              <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=M7vU1rYAAAAJ">Shuai Li</a>, 
              <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=JzBbU18AAAAJ">Yuzhi Wang</a>,
              <a href="https://scholar.google.com/citations?user=skQCiQQAAAAJ&hl=zh-CN">Xueming Qian</a>
              <br>
              <em>ICCV</em>, 2023
              <br>
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_FSI_Frequency_and_Spatial_Interactive_Learning_for_Image_Restoration_in_ICCV_2023_paper.pdf">[PDF]</a>
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_FSI_Frequency_and_Spatial_Interactive_Learning_for_Image_Restoration_in_ICCV_2023_paper.pdf">[ArXiv]</a>
	          <a href="https://github.com/ChengxuLiu/FSI">[Code]</a>
              <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Liu_FSI_Frequency_and_Spatial_Interactive_Learning_for_Image_Restoration_in_ICCV_2023_supplemental.pdf">[Supp]</a>
              <p>We introduce a new perspective to handle various diffraction in UDC images by jointly exploring the feature restoration in the frequency and spatial domains, and present a Frequency and Spatial Interactive Learning Network (FSI).</p>
            </td>
          </tr>

          <tr> 
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/CSDA.png"><img src="images/CSDA.png" alt="CSDA" width="256" height="160"></a>
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>CSDA: Learning Category-Scale Joint Feature for Domain Adaptive Object Detection</papertitle>
              </a>
              <br>
              Changlong Gao*,
              <strong>Chengxu Liu*</strong>, 
              Yujie Dun,
              <a href="https://scholar.google.com/citations?user=skQCiQQAAAAJ&hl=zh-CN">Xueming Qian</a>
              <br>
              <em>ICCV</em>, 2023
              <br>
              <a href="">[PDF]</a>
              <a href="">[ArXiv]</a>
	          <a href="">[Code]</a>
              <a href="">[Supp]</a>
              <p>For better category-level feature alignment, we propose a novel DAOD framework of joint category and scale information, dubbed CSDA, such a design enables effective object learning for different scales.</p>
            </td>
          </tr>

          <tr bgcolor="#ffffd0"> 
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/TTVFI.png"><img src="images/TTVFI.png" alt="TTVFI" width="256" height="160"></a>
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10215337">
                <papertitle>TTVFI: Learning Trajectory-Aware Transformer for Video Frame Interpolation</papertitle>
              </a>
              <br>
              <strong>Chengxu Liu</strong>, 
              <a href="https://www.microsoft.com/en-us/research/people/huayan/">Huan Yang</a>,
              <a href="https://www.microsoft.com/en-us/research/people/jianf/">Jianlong Fu</a>,
              <a href="https://scholar.google.com/citations?user=skQCiQQAAAAJ&hl=zh-CN">Xueming Qian</a>
              <br>
              <em>IEEE TIP</em>, 2023
              <br>
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10215337">[PDF]</a>
              <a href="https://arxiv.org/pdf/2207.09048.pdf">[arXiv]</a>
	      	  <a href="https://github.com/ChengxuLiu/TTVFI">[Code]</a>
              <p>We propose a novel Trajectory-aware Transformer for Video Frame Interpolation (TTVFI), which formulate the warped features with inconsistent motions as query tokens, and formulate relevant regions in a motion trajectory from two original consecutive frames into keys and values. </p>
            </td>
          </tr>

          <tr bgcolor="#ffffd0"> 
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/4DLUT.png"><img src="images/4DLUT.png" alt="4DLUT" width="256" height="160"></a>
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10226494">
                <papertitle>4D LUT: Learnable Context-Aware 4D Lookup Table for Image Enhancement</papertitle>
              </a>
              <br>
              <strong>Chengxu Liu</strong>, 
              <a href="https://www.microsoft.com/en-us/research/people/huayan/">Huan Yang</a>,
              <a href="https://www.microsoft.com/en-us/research/people/jianf/">Jianlong Fu</a>,
              <a href="https://scholar.google.com/citations?user=skQCiQQAAAAJ&hl=zh-CN">Xueming Qian</a>
              <br>
              <em>IEEE TIP</em>, 2023
              <br>
              <!-- <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10226494">[PDF]</a> -->
              <!-- <a href="https://arxiv.org/pdf/2209.01749.pdf">[arXiv]</a> -->
	      	  <!-- <a href="https://github.com/ChengxuLiu/4DLUT">[Code]</a> -->
              <p>We propose a novel learnable context-aware 4-dimensional lookup table (4D LUT), which achieves content-dependent enhancement of different contents in each image via adaptively learning of photo context.</p>
            </td>
          </tr>

          <tr> 
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/AJENet.png"><img src="images/AJENet.png" alt="AJENet" width="256" height="160"></a>
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10184096">
                <papertitle>AJENet: Adaptive Joints Enhancement Network for Abnormal Behavior Detection in Office Scenario</papertitle>
              </a>
              <br>
              <strong>Chengxu Liu</strong>,
              Yaru Zhang, 
              <a href="https://gr.xjtu.edu.cn/en/web/xueyao">Yao Xue</a>,
              <a href="https://scholar.google.com/citations?user=skQCiQQAAAAJ&hl=zh-CN">Xueming Qian</a>
              <br>
              <em>IEEE TCSVT</em>, 2023
              <br>
              <!-- <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10184096">[PDF]</a> -->
              <p>We focus on human joints and take one step further to enable effective behavior characteristics learning in office scenarios. In particular, we propose a novel Adaptive Joints Enhancement Network (AJENet).</p>
            </td>
          </tr>

          <tr> 
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/ClassAD.png"><img src="images/ClassAD.png" alt="ClassAD" width="256" height="160"></a>
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.sciencedirect.com/science/article/pii/S0950705123000011">
                <papertitle>Anomaly Detection Framework for Unmanned Vending Machines</papertitle>
              </a>
              <br>
              <a href="https://www.aminer.org/profile/zongyang-da/6447f4b9e3c28ee84c23b312">Zongyang Da</a>,
              Yujie Dun,
              <strong>Chengxu Liu</strong>, 
              <a href="https://www.semanticscholar.org/author/Yuanzhi-Liang/72322311">Yuanzhi Liang</a>,
              <a href="https://gr.xjtu.edu.cn/en/web/xueyao">Yao Xue</a>,
              <a href="https://scholar.google.com/citations?user=skQCiQQAAAAJ&hl=zh-CN">Xueming Qian</a>
              <br>
              <em>KBS</em>, 2023
              <br>
              <!-- <a href="https://www.sciencedirect.com/science/article/pii/S0950705123000011">[PDF]</a> -->
              <p>We propose an unmanned retail anomaly detection method based on deep convolutional neural networks (CNNs) called the complexity-classification anomaly detection (ClassAD) framework.</p>
            </td>
          </tr>

          <tr> 
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/PRUVM.png"><img src="images/PRUVM.png" alt="PRUVM" width="256" height="160"></a>
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9810872">
                <papertitle>Product Recognition for Unmanned Vending Machines</papertitle>
              </a>
              <br>
              <strong>Chengxu Liu</strong>, 
              <a href="https://www.aminer.org/profile/zongyang-da/6447f4b9e3c28ee84c23b312">Zongyang Da</a>,
              <a href="https://www.semanticscholar.org/author/Yuanzhi-Liang/72322311">Yuanzhi Liang</a>,
              <a href="https://gr.xjtu.edu.cn/en/web/xueyao">Yao Xue</a>,
              <a href="http://guoshuaizhao.com/">Guoshuai Zhao</a>,
              <a href="https://scholar.google.com/citations?user=skQCiQQAAAAJ&hl=zh-CN">Xueming Qian</a>
              <br>
              <em>IEEE TNNLS</em>, 2022
              <br>
              <!-- <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9810872">[PDF]</a> -->
              <!-- <a href="https://arxiv.org/abs/2211.08887">[arXiv]</a> -->
	            <!-- <a href="https://github.com/OpenPerceptionX/maskalign">[Code]</a> -->
              <p>We propose a method for large-scale categories product recognition based on intelligent UVMs. The highlights of our method are mine potential similarity between large-scale category products and optimization through hierarchical multigranularity labels</p>
            </td>
          </tr>

          <tr bgcolor="#ffffd0"> 
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/TTVSR.png"><img src="images/TTVSR.png" alt="TTVSR" width="256" height="160"></a>
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Learning_Trajectory-Aware_Transformer_for_Video_Super-Resolution_CVPR_2022_paper.pdf">
                <papertitle>Learning Trajectory-Aware Transformer for Video Super-Resolution</papertitle>
              </a>
              <br> 
              <strong>Chengxu Liu</strong>, 
              <a href="https://www.microsoft.com/en-us/research/people/huayan/">Huan Yang</a>,
              <a href="https://www.microsoft.com/en-us/research/people/jianf/">Jianlong Fu</a>,
              <a href="https://scholar.google.com/citations?user=skQCiQQAAAAJ&hl=zh-CN">Xueming Qian</a>
              <br>
              <em>CVPR (<font color="red"><b>Oral presentation</b></font>)</em>, 2022
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Learning_Trajectory-Aware_Transformer_for_Video_Super-Resolution_CVPR_2022_paper.pdf">[PDF]</a>
              <a href="https://arxiv.org/pdf/2204.04216">[arXiv]</a>
              <a href="https://github.com/researchmm/TTVSR">[Code]</a>
              <a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Liu_Learning_Trajectory-Aware_Transformer_CVPR_2022_supplemental.pdf">[Supp]</a>
              <p>We propose a novel Trajectory-aware Transformer for Video Super-Resolution (TTVSR), which formulate video frames into several pre-aligned trajectories which consist of continuous visual tokens. For a query token, self-attention is only learned on relevant visual tokens along spatio-temporal trajectories.</p>
            </td>
          </tr>

          <tr> 
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/AFN.png"><img src="images/AFN.png" alt="AFN" width="256" height="160"></a>
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9179998">
                <papertitle>Food and Ingredient Joint Learning for Fine-Grained Recognition</papertitle>
              </a>
              <br> 
              <strong>Chengxu Liu</strong>, 
              <a href="https://www.semanticscholar.org/author/Yuanzhi-Liang/72322311">Yuanzhi Liang</a>,
              <a href="https://gr.xjtu.edu.cn/en/web/xueyao">Yao Xue</a>,
              <a href="https://scholar.google.com/citations?user=skQCiQQAAAAJ&hl=zh-CN">Xueming Qian</a>,
              <a href="https://www.microsoft.com/en-us/research/people/jianf/">Jianlong Fu</a>
              <br>
              <em>IEEE TCSVT</em>, 2021
              <br>
         
              <p>We propose an Attention Fusion Network (AFN) and Food-Ingredient Joint Learning module for fine-grained food and ingredients recognition.</p>
            </td>
          </tr>

          

        </tbody></table>

          <hr>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Honors and Awards</heading>
                        <li><b>[2023/08]</b> 1st Prize in Northwest, Postgraduate Electronics Design Contest, Ministry of Education of China.</li>
                        <li><b>[2022/12]</b> Pacesetter for Postgraduate Student (The <font color="red"><b>highest honor in XJTU</b></font>, 16 Ph.D. per year), Xi'an Jiaotong University.</li>
                        <li><b>[2022/12]</b> National Scholarship, Ministry of Education of China.</li>          
                        <li><b>[2022/11]</b> 2nd Prize, 34th "Tengfei Cup" Innovation and Entrepreneurship Competition, Xi'an Jiaotong University.</li>
                        <li><b>[2022/11]</b> 3rd Prize, 34th "Tengfei Cup" Innovation and Entrepreneurship Competition, Xi'an Jiaotong University.</li>
                        <li><b>[2022/08]</b> National Most Commercially Valuable Award, Postgraduate Electronics Design Contest, Chinese Institute of Electronics (<font color="red"><b>8/5700+, 0.14\% award rate</b></font>).</li>
                        <li><b>[2022/08]</b> National 1st Prize, Postgraduate Electronics Design Contest, Chinese Institute of Electronics (<font color="red"><b>32/5700+, 0.56\% award rate</b></font>).</li>
                        <li><b>[2022/08]</b> 1st Prize in Northwest, Postgraduate Electronics Design Contest, Ministry of Education of China.</li>
                        <li><b>[2022/06]</b> 2nd Prize, Postgraduate Internet+ Contest, Ministry of Education of Shannxi. </li>
						<li><b>[2022/05]</b> Stars of Tomorrow Internship Program, Microsoft Research Asia.</li>
                        <li><b>[2020/10]</b> Excellent Graduate Student, Xi'an Jiaotong University.</li>
                        <li><b>[2020/10]</b> Principal Scholarship of Postgraduates, Xi'an Jiaotong University.</li>
                        <li><b>[2019/07]</b> Outstanding Graduate Cadre, Xi'an Jiaotong University.</li>
            </td>
          </tr>
        </tbody></table>

          <hr>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Reviewer</heading>
                       <li> Conference: ICCV 2023, ECCV 2022, CVPR 2022,2023, ICME 2022,2023.</li>
                       <li> Journal: IEEE TIP, IEEE TMM, IEEE TNNLS, IEEE TCSVT, KBS.</li>
            </td>
          </tr>
        </tbody></table>

          <hr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Based on <a href="https://jonbarron.info/">Jon Barron's website</a>.
                
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
